{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc73875-2796-4ffb-9428-f972a1b8c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "import sentencepiece as spm\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bdfe878-a5cc-47da-a799-b79e23360164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c010bf8-40ff-4772-8498-44408e1b1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb51ef6-77d7-489b-ae9b-a6471f473130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wmt14():\n",
    "    print(\"Downloading WMT14 DE-EN dataset...\")\n",
    "    # Download the dataset\n",
    "    dataset = load_dataset(\"wmt14\", \"de-en\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775e16cd-05df-4083-9947-e8b1e37605f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 512  # Embedding dimension\n",
    "NHEAD = 8  # Number of attention heads\n",
    "NUM_ENCODER_LAYERS = 6  # Number of encoder layers\n",
    "NUM_DECODER_LAYERS = 6  # Number of decoder layers\n",
    "DIM_FEEDFORWARD = 2048  # Dimension of the feedforward network\n",
    "DROPOUT = 0.1  # Dropout rate\n",
    "BATCH_SIZE = 32  # Batch size for training\n",
    "ACCUMULATION_STEPS = 4  # Gradient accumulation steps\n",
    "LEARNING_RATE = 0.0001  # Initial learning rate\n",
    "BETAS = (0.9, 0.98)  # Adam optimizer betas\n",
    "EPS = 1e-9  # Adam optimizer epsilon\n",
    "WEIGHT_DECAY = 0.0001  # Weight decay for regularization\n",
    "LABEL_SMOOTHING = 0.1  # Label smoothing factor\n",
    "MAX_SEQ_LENGTH = 100  # Maximum sequence length\n",
    "WARMUP_STEPS = 4000  # Warmup steps for learning rate scheduler\n",
    "MAX_EPOCHS = 30  # Maximum number of epochs\n",
    "CLIP_GRAD = 1.0  # Gradient clipping value\n",
    "PATIENCE = 5  # Patience for early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6fa7775-4bb4-4f71-9bcf-7b17f18ba054",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = '<s>'\n",
    "EOS_TOKEN = '</s>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "SPECIAL_TOKENS = [UNK_TOKEN, PAD_TOKEN, BOS_TOKEN, EOS_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4c47e90-a06a-4274-8346-2142491ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_huggingface_tokenizers():\n",
    "    \"\"\"Use pre-trained tokenizers from HuggingFace\"\"\"\n",
    "    from transformers import MarianTokenizer\n",
    "    \n",
    "    print(\"Loading pre-trained MarianMT tokenizers...\")\n",
    "    tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")\n",
    "    \n",
    "    # Create a simplified vocabulary wrapper to match our expected interface\n",
    "    class HFVocabulary:\n",
    "        def __init__(self, tokenizer):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.pad_idx = tokenizer.pad_token_id\n",
    "            self.bos_idx = tokenizer.bos_token_id\n",
    "            self.eos_idx = tokenizer.eos_token_id\n",
    "            self.unk_idx = tokenizer.unk_token_id\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.tokenizer)\n",
    "        \n",
    "        def encode(self, text):\n",
    "            \"\"\"Convert text to token IDs\"\"\"\n",
    "            return self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        def encode_with_special_tokens(self, text, add_bos=True, add_eos=True):\n",
    "            \"\"\"Encode with optional BOS/EOS tokens\"\"\"\n",
    "            # Manually handle BOS and EOS to avoid warnings about unrecognized parameters\n",
    "            ids = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "            \n",
    "            if add_bos:\n",
    "                ids = [self.bos_idx] + ids\n",
    "            if add_eos:\n",
    "                ids = ids + [self.eos_idx]\n",
    "            \n",
    "            return ids\n",
    "        \n",
    "        def decode(self, ids):\n",
    "            \"\"\"Convert token IDs to text\"\"\"\n",
    "            return self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "        \n",
    "        def token_to_id(self, token):\n",
    "            \"\"\"Get ID for a token\"\"\"\n",
    "            return self.tokenizer.convert_tokens_to_ids(token)\n",
    "        \n",
    "        def id_to_token(self, id):\n",
    "            \"\"\"Get token for an ID\"\"\"\n",
    "            return self.tokenizer.convert_ids_to_tokens(id)\n",
    "    \n",
    "    return HFVocabulary(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141e162f-8eb4-4f36-b35b-8eb9235f324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPVocabulary:\n",
    "    def __init__(self, sp_model):\n",
    "        self.sp = sp_model\n",
    "        self.pad_idx = self.sp.piece_to_id(PAD_TOKEN)\n",
    "        self.bos_idx = self.sp.piece_to_id(BOS_TOKEN)\n",
    "        self.eos_idx = self.sp.piece_to_id(EOS_TOKEN)\n",
    "        self.unk_idx = self.sp.piece_to_id(UNK_TOKEN)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sp.get_piece_size()\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        return self.sp.encode(text, out_type=int)\n",
    "    \n",
    "    def encode_with_special_tokens(self, text, add_bos=True, add_eos=True):\n",
    "        \"\"\"Encode with optional BOS/EOS tokens\"\"\"\n",
    "        ids = self.encode(text)\n",
    "        if add_bos:\n",
    "            ids = [self.bos_idx] + ids\n",
    "        if add_eos:\n",
    "            ids = ids + [self.eos_idx]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs to text\"\"\"\n",
    "        return self.sp.decode(ids)\n",
    "    \n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"Get ID for a token\"\"\"\n",
    "        return self.sp.piece_to_id(token)\n",
    "    \n",
    "    def id_to_token(self, id):\n",
    "        \"\"\"Get token for an ID\"\"\"\n",
    "        return self.sp.id_to_piece(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf40abc3-f396-40ea-92b4-9acdfa2d85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=MAX_SEQ_LENGTH):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get source and target texts\n",
    "        try:\n",
    "            src_text = self.data[idx]['translation']['de']\n",
    "            tgt_text = self.data[idx]['translation']['en']\n",
    "            \n",
    "            # Skip empty strings\n",
    "            if not src_text or not tgt_text:\n",
    "                # Return a simple placeholder with the right format\n",
    "                return torch.tensor([self.vocab.bos_idx, self.vocab.eos_idx]), \\\n",
    "                       torch.tensor([self.vocab.bos_idx, self.vocab.eos_idx])\n",
    "            \n",
    "            # Encode source and target texts\n",
    "            src_ids = self.vocab.encode_with_special_tokens(src_text, add_bos=False, add_eos=True)\n",
    "            tgt_ids = self.vocab.encode_with_special_tokens(tgt_text, add_bos=True, add_eos=True)\n",
    "            \n",
    "            # Handle empty encodings or None\n",
    "            if not src_ids or src_ids is None:\n",
    "                src_ids = [self.vocab.bos_idx, self.vocab.eos_idx]\n",
    "            if not tgt_ids or tgt_ids is None:\n",
    "                tgt_ids = [self.vocab.bos_idx, self.vocab.eos_idx]\n",
    "            \n",
    "            # Truncate if necessary\n",
    "            if len(src_ids) > self.max_len:\n",
    "                src_ids = src_ids[:self.max_len-1] + [self.vocab.eos_idx]\n",
    "            if len(tgt_ids) > self.max_len:\n",
    "                tgt_ids = tgt_ids[:self.max_len-1] + [self.vocab.eos_idx]\n",
    "                \n",
    "            return torch.tensor(src_ids), torch.tensor(tgt_ids)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx}: {e}\")\n",
    "            # Return a simple pair of tensors to maintain data flow\n",
    "            return torch.tensor([self.vocab.bos_idx, self.vocab.eos_idx]), \\\n",
    "                   torch.tensor([self.vocab.bos_idx, self.vocab.eos_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b429ca71-0bed-42e0-8190-8c819a6c31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_idx):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_batch.append(src)\n",
    "        tgt_batch.append(tgt)\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_batch = pad_sequence(src_batch, padding_value=pad_idx, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=pad_idx, batch_first=True)\n",
    "    \n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1405d840-f7d1-4534-8865-ecda2458668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(dataset, vocab, batch_size=BATCH_SIZE, val_split=0.05, max_len=MAX_SEQ_LENGTH):\n",
    "    \"\"\"Create training and validation DataLoader objects\"\"\"\n",
    "    # Filter out examples with empty translations\n",
    "    filtered_data = []\n",
    "    for example in dataset['train']:\n",
    "        if (example['translation']['de'] and \n",
    "            example['translation']['en'] and \n",
    "            len(example['translation']['de']) > 5 and  # Ensure minimal sentence length\n",
    "            len(example['translation']['en']) > 5):\n",
    "            filtered_data.append(example)\n",
    "    \n",
    "    # Create a custom dataset dict\n",
    "    filtered_dataset = {'train': filtered_data}\n",
    "    if 'test' in dataset:\n",
    "        # Also filter test data\n",
    "        filtered_test = []\n",
    "        for example in dataset['test']:\n",
    "            if (example['translation']['de'] and \n",
    "                example['translation']['en'] and \n",
    "                len(example['translation']['de']) > 5 and\n",
    "                len(example['translation']['en']) > 5):\n",
    "                filtered_test.append(example)\n",
    "        filtered_dataset['test'] = filtered_test\n",
    "    else:\n",
    "        filtered_dataset['test'] = []  # Empty test set if not present\n",
    "    \n",
    "    print(f\"Filtered training examples: {len(filtered_dataset['train'])} (from {len(dataset['train'])})\")\n",
    "    if 'test' in dataset:\n",
    "        print(f\"Filtered test examples: {len(filtered_dataset['test'])} (from {len(dataset['test'])})\")\n",
    "    \n",
    "    # Create a TranslationDataset\n",
    "    full_dataset = TranslationDataset(filtered_dataset['train'], vocab, max_len)\n",
    "    \n",
    "    # Split into train and validation\n",
    "    val_size = int(len(full_dataset) * val_split)\n",
    "    train_size = len(full_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Get test dataset if available\n",
    "    test_dataset = None\n",
    "    if len(filtered_dataset['test']) > 0:\n",
    "        test_dataset = TranslationDataset(filtered_dataset['test'], vocab, max_len)\n",
    "    else:\n",
    "        # Use part of validation as test if no test set available\n",
    "        additional_val_size = int(val_size * 0.5)\n",
    "        val_size = val_size - additional_val_size\n",
    "        val_dataset, test_dataset = random_split(val_dataset, [val_size, additional_val_size])\n",
    "    \n",
    "    # Configure DataLoader parameters to handle errors gracefully\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'collate_fn': lambda batch: collate_fn(batch, vocab.pad_idx),\n",
    "        'pin_memory': True,\n",
    "        'num_workers': 12, \n",
    "        'persistent_workers': True if torch.cuda.is_available() else False,\n",
    "    }\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        **dataloader_kwargs\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        shuffle=False,\n",
    "        **dataloader_kwargs\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle=False,\n",
    "        **dataloader_kwargs\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfe1310-506d-4009-ac7e-e8db2dbca2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate sin and cos terms\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca472cd1-6cb3-4371-bea0-8e712436767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=D_MODEL, nhead=NHEAD, \n",
    "                 num_encoder_layers=NUM_ENCODER_LAYERS, \n",
    "                 num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "                 dim_feedforward=DIM_FEEDFORWARD, dropout=DROPOUT):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        # Store dimensions\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Transformer layers from PyTorch\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_encoder_layers,\n",
    "            norm=encoder_norm\n",
    "        )\n",
    "        \n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=num_decoder_layers,\n",
    "            norm=decoder_norm\n",
    "        )\n",
    "        \n",
    "        # Final linear layer for output\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize parameters with Glorot/Xavier initialization\n",
    "        self._reset_parameters()\n",
    "        \n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initialize parameters with appropriate scaling\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "        # Special initialization for embedding\n",
    "        nn.init.normal_(self.embedding.weight, mean=0, std=self.d_model ** -0.5)\n",
    "        \n",
    "    def create_padding_mask(self, src, pad_idx):\n",
    "        \"\"\"Create mask for padding tokens (True where pad token)\"\"\"\n",
    "        return (src == pad_idx).to(device)\n",
    "        \n",
    "    def create_look_ahead_mask(self, size):\n",
    "        \"\"\"Create mask to prevent attention to future tokens\"\"\"\n",
    "        # Create an upper triangular matrix with 1s\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
    "        return mask.to(device)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"Forward pass of the transformer model\"\"\"\n",
    "        # Get padding token index\n",
    "        pad_idx = self.embedding.padding_idx if self.embedding.padding_idx is not None else 0\n",
    "        \n",
    "        # Create masks\n",
    "        src_padding_mask = self.create_padding_mask(src, pad_idx)  # [batch_size, src_len]\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt[:, :-1], pad_idx)  # [batch_size, tgt_len-1]\n",
    "        tgt_look_ahead_mask = self.create_look_ahead_mask(tgt.size(1)-1)  # [tgt_len-1, tgt_len-1]\n",
    "        \n",
    "        # Embed and apply positional encoding\n",
    "        src_embedded = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        \n",
    "        tgt_embedded = self.embedding(tgt[:, :-1]) * math.sqrt(self.d_model)\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        memory = self.transformer_encoder(\n",
    "            src=src_embedded,\n",
    "            src_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Transformer decoder\n",
    "        output = self.transformer_decoder(\n",
    "            tgt=tgt_embedded,\n",
    "            memory=memory,\n",
    "            tgt_mask=tgt_look_ahead_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Final output projection\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31c1d559-32f2-474b-be94-6653ed27f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamLR:\n",
    "    \"\"\"Learning rate scheduler from 'Attention is All You Need'\"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self._step = 0\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"Update parameters and learning rate\"\"\"\n",
    "        self._step += 1\n",
    "        rate = self._get_lr()\n",
    "        self._rate = rate\n",
    "        \n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "            \n",
    "    def _get_lr(self):\n",
    "        \"\"\"Calculate learning rate according to the formula\"\"\"\n",
    "        step = self._step\n",
    "        return self.factor * (self.d_model ** -0.5) * min(step ** -0.5, step * self.warmup_steps ** -1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16032cbf-6f0d-4e2a-a794-a01374ffad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, criterion, clip_grad=CLIP_GRAD, accumulation_steps=ACCUMULATION_STEPS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    processed_batches = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(progress_bar):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        # Calculate loss\n",
    "        output = output.contiguous().view(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)  # Shift right to align with output\n",
    "        loss = criterion(output, tgt) / accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Track total loss\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        processed_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': total_loss / processed_batches})\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "    \n",
    "    return total_loss / processed_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e80b37b-5cb1-4fb4-b072-312fab610e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    processed_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt)\n",
    "            \n",
    "            # Calculate loss\n",
    "            output = output.contiguous().view(-1, output.shape[-1])\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            \n",
    "            # Track total loss\n",
    "            total_loss += loss.item()\n",
    "            processed_batches += 1\n",
    "    \n",
    "    return total_loss / processed_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4904a9-8212-42d7-b1d9-d3ef403168ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src, max_len=100, beam_size=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Move source tensor to device\n",
    "    src = src.to(device)\n",
    "    \n",
    "    # Encode source sequence\n",
    "    with torch.no_grad():\n",
    "        src_embedded = model.embedding(src) * math.sqrt(model.d_model)\n",
    "        src_embedded = model.positional_encoding(src_embedded)\n",
    "        \n",
    "        src_padding_mask = model.create_padding_mask(\n",
    "            src, model.embedding.padding_idx if model.embedding.padding_idx is not None else 0\n",
    "        )\n",
    "        \n",
    "        memory = model.transformer_encoder(\n",
    "            src=src_embedded,\n",
    "            src_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "    \n",
    "    # Start with BOS token\n",
    "    bos_idx = model.embedding.padding_idx + 1 if model.embedding.padding_idx is not None else 1\n",
    "    ys = torch.ones(1, 1).fill_(bos_idx).type_as(src).long().to(device)\n",
    "    \n",
    "    # Initialize beams: (sequence, score)\n",
    "    beams = [(ys, 0.0)]\n",
    "    completed_beams = []\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            # If this sequence ended with EOS\n",
    "            if seq[0, -1].item() == bos_idx + 1:\n",
    "                completed_beams.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            # Predict next tokens\n",
    "            with torch.no_grad():\n",
    "                tgt_embedded = model.embedding(seq) * math.sqrt(model.d_model)\n",
    "                tgt_embedded = model.positional_encoding(tgt_embedded)\n",
    "                \n",
    "                tgt_mask = model.create_look_ahead_mask(seq.size(1))\n",
    "                tgt_padding_mask = model.create_padding_mask(\n",
    "                    seq, model.embedding.padding_idx if model.embedding.padding_idx is not None else 0\n",
    "                )\n",
    "                \n",
    "                out = model.transformer_decoder(\n",
    "                    tgt=tgt_embedded,\n",
    "                    memory=memory,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    tgt_key_padding_mask=tgt_padding_mask,\n",
    "                    memory_key_padding_mask=src_padding_mask\n",
    "                )\n",
    "                \n",
    "                out = model.fc_out(out[:, -1])\n",
    "                prob = F.log_softmax(out, dim=-1)\n",
    "            \n",
    "            # Get top-k candidates\n",
    "            topk_prob, topk_idx = prob[0].topk(beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                next_token = topk_idx[i].unsqueeze(0).unsqueeze(0)\n",
    "                next_score = score + topk_prob[i].item()\n",
    "                next_seq = torch.cat([seq, next_token], dim=1)\n",
    "                candidates.append((next_seq, next_score))\n",
    "        \n",
    "        # Keep only top beams\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "        # Early stopping if all beams end with EOS\n",
    "        if all(beam[0][0, -1].item() == bos_idx + 1 for beam in beams):\n",
    "            break\n",
    "    \n",
    "    # Add any remaining beams to completed\n",
    "    for seq, score in beams:\n",
    "        if seq[0, -1].item() != bos_idx + 1:\n",
    "            seq = torch.cat([seq, torch.ones(1, 1).fill_(bos_idx + 1).type_as(src).to(device)], dim=1)\n",
    "        completed_beams.append((seq, score))\n",
    "    \n",
    "    # Return the highest scoring beam\n",
    "    if completed_beams:\n",
    "        return max(completed_beams, key=lambda x: x[1])[0]\n",
    "    else:\n",
    "        return beams[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd356a08-a262-4413-b912-20421fa3a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, \n",
    "               max_epochs=MAX_EPOCHS, patience=PATIENCE, save_path=\"best_model.pt\"):\n",
    "    # Initialize tracking variables\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(max_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate time taken\n",
    "        epoch_mins, epoch_secs = divmod(time.time() - start_time, 60)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, save_path)\n",
    "            print(f\"New best model saved with validation loss: {val_loss:.3f}\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a67abe4-58f8-4cde-b542-e74ec57d67dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading WMT14 DE-EN dataset...\n",
      "Training examples: 4508785\n",
      "Test examples: 3003\n"
     ]
    }
   ],
   "source": [
    "dataset = download_wmt14()\n",
    "print(f\"Training examples: {len(dataset['train'])}\")\n",
    "print(f\"Test examples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29ead07e-04ac-41d2-9e78-24f0dcf5eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained MarianMT tokenizers...\n",
      "HuggingFace vocabulary size: 58101\n"
     ]
    }
   ],
   "source": [
    "vocab = use_huggingface_tokenizers()\n",
    "print(f\"HuggingFace vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e21ba5f2-c87a-4200-bbb1-80f5771e7ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered training examples: 4502068 (from 4508785)\n",
      "Filtered test examples: 3002 (from 3003)\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = create_dataloaders(dataset, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "408f5547-c3e7-475c-a86b-47d7c8966c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 103,694,069 parameters\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(len(vocab)).to(device)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "743b689f-6ef3-488a-bc76-0a600371db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.padding_idx = vocab.pad_idx\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx, label_smoothing=LABEL_SMOOTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96843e66-a4c8-41c1-8645-fa63173fbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    betas=BETAS, \n",
    "    eps=EPS,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scheduler = NoamLR(optimizer, D_MODEL, WARMUP_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0172d2d3-1260-49a5-9fe6-714c5d572511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a04dd3cdd8d4b0d939ac700d98381e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/133656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/tmp/ipykernel_232698/790162208.py\", line 38, in __getitem__\n    return torch.tensor(src_ids), torch.tensor(tgt_ids)\n                                  ^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Could not infer dtype of NoneType\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sunil/.local/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sunil/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sunil/.local/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_232698/790162208.py\", line 43, in __getitem__\n    return torch.tensor([self.vocab.bos_idx, self.vocab.eos_idx]), \\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Could not infer dtype of NoneType\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, criterion, max_epochs, patience, save_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, criterion, clip_grad, accumulation_steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Forward pass\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/tmp/ipykernel_232698/790162208.py\", line 38, in __getitem__\n    return torch.tensor(src_ids), torch.tensor(tgt_ids)\n                                  ^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Could not infer dtype of NoneType\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sunil/.local/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sunil/.local/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sunil/.local/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_232698/790162208.py\", line 43, in __getitem__\n    return torch.tensor([self.vocab.bos_idx, self.vocab.eos_idx]), \\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Could not infer dtype of NoneType\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model(model, train_loader, val_loader, optimizer, scheduler, criterion)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "185c916c-bba2-429b-b38c-45b530b7509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = \"final_wmt14_de_en_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': len(vocab),\n",
    "    'model_config': {\n",
    "        'd_model': D_MODEL,\n",
    "        'nhead': NHEAD,\n",
    "        'num_encoder_layers': NUM_ENCODER_LAYERS,\n",
    "        'num_decoder_layers': NUM_DECODER_LAYERS,\n",
    "        'dim_feedforward': DIM_FEEDFORWARD,\n",
    "        'dropout': DROPOUT\n",
    "    }\n",
    "}, final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "533fb5f4-8e0b-44a5-80b2-db02b12306d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model.save(\"spm_wmt14_de_en.model\")\n",
    "\n",
    "print(f\"Model saved to {final_model_path}\")\n",
    "print(\"SentencePiece model saved to spm_wmt14_de_en.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f51de38f-2b95-4e33-ac6a-d0698e0a606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 10564101 parameters\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    len(de_vocab), \n",
    "    len(en_vocab), \n",
    "    d_model=D_MODEL, \n",
    "    nhead=NHEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS, \n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    d_ff=D_FF, \n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bacfb68b-aad6-43cd-9f49-0a5f9be70aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(step, d_model, warmup_steps):\n",
    "    \"\"\"Learning rate schedule as described in the paper\"\"\"\n",
    "    arg1 = step ** -0.5\n",
    "    arg2 = step * (warmup_steps ** -1.5)\n",
    "    return (d_model ** -0.5) * min(arg1, arg2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=BETAS, eps=EPS)\n",
    "scheduler = LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: lr_schedule(step + 1, D_MODEL, WARMUP_STEPS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1357123f-b53c-4965-937e-d2ea4b95152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=de_vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc72297f-964f-46f7-a63b-c0e7768e0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src, max_len, start_token, end_token, pad_token, beam_size=5):\n",
    "    model.eval()\n",
    "    \n",
    "    src = src.to(device)\n",
    "    src_mask = create_padding_mask(src, pad_token)\n",
    "    \n",
    "    # Encode source sequence\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    # Initialize with start token\n",
    "    ys = torch.ones(1, 1).fill_(start_token).type_as(src).to(device)\n",
    "    \n",
    "    # Initial beam\n",
    "    beams = [(ys, 0.0)]  # (sequence, score)\n",
    "    completed_beams = []\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        candidates = []\n",
    "        \n",
    "        for seq, score in beams:\n",
    "            if seq[0, -1].item() == end_token:\n",
    "                completed_beams.append((seq, score))\n",
    "                continue\n",
    "                \n",
    "            # Create masks\n",
    "            tgt_mask = create_look_ahead_mask(seq.size(1)).to(device)\n",
    "            memory_mask = src_mask\n",
    "            \n",
    "            # Decode one step\n",
    "            out = model.decode(seq, memory, tgt_mask, memory_mask)\n",
    "            prob = F.log_softmax(model.generator(out[:, -1]), dim=-1)\n",
    "            \n",
    "            # Get top k\n",
    "            topk_prob, topk_idx = prob.topk(beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                next_token = topk_idx[0, i].item()\n",
    "                next_score = score + topk_prob[0, i].item()\n",
    "                next_seq = torch.cat([seq, torch.ones(1, 1).type_as(src).fill_(next_token).to(device)], dim=1)\n",
    "                candidates.append((next_seq, next_score))\n",
    "        \n",
    "        # Keep only top beams\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "        # Stop if all beams end with end token\n",
    "        if all(beam[0][0, -1].item() == end_token for beam in beams):\n",
    "            break\n",
    "    \n",
    "    # Add any remaining beams to completed\n",
    "    for seq, score in beams:\n",
    "        if seq[0, -1].item() != end_token:\n",
    "            seq = torch.cat([seq, torch.ones(1, 1).type_as(src).fill_(end_token).to(device)], dim=1)\n",
    "        completed_beams.append((seq, score))\n",
    "    \n",
    "    # Return the highest scoring beam\n",
    "    return max(completed_beams, key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "71b132c3-8259-4aba-8fa8-e79bd6f96bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence, src_tokenizer, tgt_vocab, src_vocab, max_len=50, beam_size=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input sentence\n",
    "    tokens = src_tokenizer(sentence)\n",
    "    \n",
    "    # Convert tokens to indices\n",
    "    token_ids = [src_vocab[token] for token in tokens]\n",
    "    \n",
    "    # Create tensor\n",
    "    src = torch.tensor([token_ids]).to(device)\n",
    "    \n",
    "    # Decode using beam search\n",
    "    output = beam_search_decode(\n",
    "        model, \n",
    "        src, \n",
    "        max_len, \n",
    "        tgt_vocab[BOS_TOKEN], \n",
    "        tgt_vocab[EOS_TOKEN], \n",
    "        src_vocab[PAD_TOKEN],\n",
    "        beam_size\n",
    "    )\n",
    "    \n",
    "    # Convert output indices to tokens\n",
    "    output_tokens = [tgt_vocab.lookup_token(i) for i in output[0, 1:].tolist()]\n",
    "    \n",
    "    # Stop at EOS token\n",
    "    if tgt_vocab[EOS_TOKEN] in output_tokens:\n",
    "        output_tokens = output_tokens[:output_tokens.index(tgt_vocab[EOS_TOKEN])]\n",
    "    \n",
    "    return ' '.join(output_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
